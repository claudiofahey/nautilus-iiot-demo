{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from time import sleep, time\n",
    "import json\n",
    "import numpy as np\n",
    "import base64\n",
    "import grpc\n",
    "import logging\n",
    "import zlib\n",
    "import struct\n",
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "import pravega"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\n",
    "    From https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "def pravega_chunker_v1(payload, max_chunk_size):\n",
    "    \"\"\"Split payload into chunks of 1 MiB or less.\n",
    "    When written to Pravega, chunked events have the following 64-bit header.\n",
    "      version: value must be 1 (8-bit signed integer)\n",
    "      reserved: value must be 0 (3 8-bit signed integers)\n",
    "      chunk_index: 0-based chunk index (16-bit signed big endian integer)\n",
    "      final_chunk_index: number of chunks minus 1 (16 bit signed big endian integer)\n",
    "    \"\"\"\n",
    "    version = 1\n",
    "    max_chunk_data_size = max_chunk_size - 4096\n",
    "    chunk_list = list(chunks(payload, max_chunk_data_size))\n",
    "    final_chunk_index = len(chunk_list) - 1\n",
    "    for chunk_index, chunked_payload in enumerate(chunk_list):\n",
    "        is_final_chunk = chunk_index == final_chunk_index\n",
    "        header = struct.pack('!bxxxhh', version, chunk_index, final_chunk_index)\n",
    "        chunked_event = header + chunked_payload\n",
    "        yield (chunked_event, chunk_index, is_final_chunk)\n",
    "\n",
    "def encode_record(record: dict) -> bytes:\n",
    "    \"\"\"Encode the record\n",
    "    JSON is universally compatible but require images to be base64 encoded.\n",
    "    For optimal performance, other encodings should be used such as Avro or Protobuf.\n",
    "    \"\"\"\n",
    "    r = record.copy()\n",
    "    r['data'] = base64.b64encode(record['data']).decode('utf-8')\n",
    "    encoded = json.dumps(r).encode('utf-8')\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a record for every frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pravega_request_generator(data_generator, scope, stream, max_chunk_size, use_transactions):\n",
    "    for record in data_generator:\n",
    "        t = record['timestamp']\n",
    "        sleep_sec = t/1000.0 - time()\n",
    "        if sleep_sec > 0.0:\n",
    "            sleep(sleep_sec)\n",
    "        record['timestamp'] = (datetime(1970, 1, 1) + timedelta(milliseconds=t)).strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3] + 'Z'\n",
    "        payload = encode_record(record)\n",
    "        for (chunked_event, chunk_index, is_final_chunk) in pravega_chunker_v1(payload, max_chunk_size=max_chunk_size):\n",
    "            request = pravega.pb.WriteEventsRequest(\n",
    "                event=chunked_event,\n",
    "                scope=scope,\n",
    "                stream=stream,\n",
    "                use_transaction=use_transactions,\n",
    "                commit=is_final_chunk and use_transactions,\n",
    "                )\n",
    "            # logging.info(request)\n",
    "            yield request\n",
    "        if True:\n",
    "            record_to_log = record.copy()\n",
    "            # record_to_log['data'] = str(record_to_log['data'][:10]) + '...'\n",
    "            del record_to_log['data']\n",
    "            record_to_log['data_len'] = len(record['data'])\n",
    "            record_to_log['payload_len'] = len(payload)\n",
    "            record_to_log['final_chunk_index'] = chunk_index\n",
    "            logging.info('%d: %s' % (record_to_log['camera'], json.dumps(record_to_log)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read one frame at a time from camera feed and create a record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(cap, include_checksum, ssrc):\n",
    "    frame_number = 0\n",
    "    while True:\n",
    "        # Capture video frame by frame\n",
    "        ret, frame = cap.read()\n",
    "        timestamp = int(time() * 1000)\n",
    "        # _, data = cv2.imencode(\".png\", frame, [cv2.IMWRITE_PNG_COMPRESSION, 0])\n",
    "        _, data = cv2.imencode(\".jpg\", frame)\n",
    "        if include_checksum:\n",
    "            # Add CRC32 checksum to allow for error detection.\n",
    "            chucksum = struct.pack('!I', zlib.crc32(data))\n",
    "            data = chucksum + frame\n",
    "        record = {\n",
    "            'timestamp': timestamp,\n",
    "            'frame_number': frame_number,\n",
    "            'camera': 0,\n",
    "            'ssrc': ssrc,\n",
    "            'data': data            \n",
    "        }\n",
    "        yield record\n",
    "        frame_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capture video from a webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "# Variables \n",
    "PRAVEGA_GATEWAY = '10.1.83.104:80'  # Pravega gateway\n",
    "PRAVEGA_SCOPE = 'examples'\n",
    "PRAVEGA_STREAM = 'video'    \n",
    "ADD_CHECKSUM = False  # Prepend the data with a checksum that can be used to detect errors    \n",
    "USE_TRANSACTIONS = False  # If true, use Pravega transactions\n",
    "MAX_CHUNK_SIZE = 1024*1024  # Maximum size of chunk (bytes)\n",
    "\n",
    "# Capture video from camera with index 0\n",
    "cap = cv2.VideoCapture(0)\n",
    "logging.debug(\"frame width is {0}; frame height is {1}\".format(cap.get(3), cap.get(4)))\n",
    "\n",
    "while(True):\n",
    "    ssrc = np.random.randint(0, 2**31)\n",
    "    data_iter = data_generator(cap, ADD_CHECKSUM, ssrc)\n",
    "    pravega_request_iter = pravega_request_generator(data_iter, PRAVEGA_SCOPE, PRAVEGA_STREAM, MAX_CHUNK_SIZE, USE_TRANSACTIONS)\n",
    "    with grpc.insecure_channel(PRAVEGA_GATEWAY) as pravega_channel:\n",
    "        pravega_client = pravega.grpc.PravegaGatewayStub(pravega_channel)\n",
    "        pravega_client.CreateScope(pravega.pb.CreateScopeRequest(scope=PRAVEGA_SCOPE))\n",
    "        pravega_client.CreateStream(pravega.pb.CreateStreamRequest(scope=PRAVEGA_SCOPE, stream=PRAVEGA_STREAM))\n",
    "        write_response = pravega_client.WriteEvents(pravega_request_iter)\n",
    "        logging.info(\"write_response=\" + str(write_response))\n",
    "\n",
    "# Realease the capture on exit\n",
    "cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
